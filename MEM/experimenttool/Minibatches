

from skimage import io
#,transform
import matplotlib.pyplot as plt
import glob
import os
import tensorflow as tf
import numpy as np
import time

path='/home/chaolu/SIDS/data'
path_1 = '/home/chaolu/SIDS/data/good' #997
path_2 = '/home/chaolu/SIDS/data/bad' #3525
#print('filenum:',len([lists for lists in os.listdir(path1) if os.path.isfile(os.path.join(path, lists))]))
#print('filenum:',len([lists for lists in os.listdir(path_2) if os.path.isfile(os.path.join(path_2, lists))]))
#Resize all pictures (w for width, h for height, c for color)
w=256
h=256
c=3


#Read the pictures
def read_img(path):
    cate=[path+'/'+x for x in os.listdir(path) if os.path.isdir(path+'/'+x)]
    print('cate:',cate)
    imgs=[]
    labels=[]
    for idx,folder in enumerate(cate):
        
        for im in glob.glob(folder+'/*.jpeg'):
            #print('reading the images:%s label: %s'%(im,idx))
            img=io.imread(im)
            
            #print('imgsize:', img.shape)
            #img_new=np.reshape(img,(256,256))
            
            #img=transform.resize(img,(w,h))
            imgs.append(img)
            labels.append(idx)
        print('labels_shape:',np.array(labels).shape)
    return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)
data,label=read_img(path)
#print('data_shape:',data.shape)
#Disrupt the order
num_example=data.shape[0]
#print('num_example:',num_example)
arr=np.arange(num_example)
np.random.shuffle(arr)
data=data[arr]
label=label[arr]
#print('label:',label)
print(label.shape)

#Divide the whole data set into training set and testing set
ratio=0.785
s=np.int(num_example*ratio)
x_train=data[:s]
y_train=label[:s]
x_val=data[s:]
y_val=label[s:]
print('y_val:',y_val)
print('y_val_shape:',y_val.shape)

#-----------------Create network----------------------
#placeholde r
x=tf.placeholder(tf.float32,shape=[None,w,h,c],name='x')
y_=tf.placeholder(tf.int32,shape=[None,],name='y_')

#1st conv layerï¼ˆ256>128)
conv1=tf.layers.conv2d(
      inputs=x,
      filters=32,
      kernel_size=[5, 5],
      padding="same",
      activation=tf.nn.relu,
      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))
pool1=tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)


#2nd conv layer(128->64)
conv2=tf.layers.conv2d(
      inputs=pool1,
      filters=64,
      kernel_size=[5, 5],
      padding="same",
      activation=tf.nn.relu,
      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))
pool2=tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

#3rd conv layer(64->32)
conv3=tf.layers.conv2d(
      inputs=pool2,
      filters=128,
      kernel_size=[3, 3],
      padding="same",
      activation=tf.nn.relu,
      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))
pool3=tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)

#4th conv layer(32->16)
conv4=tf.layers.conv2d(
      inputs=pool3,
      filters=128,
      kernel_size=[3, 3],
      padding="same",
      activation=tf.nn.relu,
      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))
pool4=tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)

re1 = tf.reshape(pool4,[-1,16*16*128])

#Full connected layer
dense1 = tf.layers.dense(inputs=re1, 
                      units=1024,  
                      activation=tf.nn.relu,
                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                      kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))

dense2= tf.layers.dense(inputs=dense1, 
                      units=512,   
                      activation=tf.nn.relu,
                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                      kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))
logits= tf.layers.dense(inputs=dense2, 
                        units=2, 
                        activation=None,
                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),
                        kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))
#---------------------------Network end---------------------------
loss=tf.losses.sparse_softmax_cross_entropy(labels=y_,logits=logits)
train_op=tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)
correct_prediction = tf.equal(tf.cast(tf.argmax(logits,1),tf.int32), y_)    
acc= tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

#Define a function that select a batch for every epoch
def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):
    assert len(inputs) == len(targets)
    if shuffle:
        indices = np.arange(len(inputs))
        np.random.shuffle(indices)
    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):
        if shuffle:
            excerpt = indices[start_idx:start_idx + batch_size]
        else:
            excerpt = slice(start_idx, start_idx + batch_size)
        yield inputs[excerpt], targets[excerpt]

#Training and testing data
n_epoch=300
batch_size=64
sess=tf.InteractiveSession()  
sess.run(tf.global_variables_initializer())

XX = range(0,n_epoch+1,1)
ACC = np.array([0.]*(n_epoch+1),'float32')
for epoch in range(1,n_epoch+1):
    start_time = time.time()
       
    #training
    train_loss, train_acc, n_batch = 0, 0, 0
    for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):
        _,err,ac=sess.run([train_op,loss,acc], feed_dict={x: x_train_a, y_: y_train_a})
        train_loss += err; train_acc += ac; n_batch += 1
    
    #validation
    val_loss, val_acc, n_batch = 0, 0, 0
    for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=True):
        err, ac = sess.run([loss,acc], feed_dict={x: x_val_a, y_: y_val_a})
        val_loss += err; val_acc += ac; n_batch += 1
        result = sess.run(correct_prediction,feed_dict={x:x_val_a,y_:y_val_a})    
        if not result:
          #one_pic = np.reshape()
          plt.suptitle()
          plt.imshow(img)
          plt.axis('off')
          plt.show()
    #print("   validation loss: %f" % (val_loss/ n_batch))
    #print("   validation acc: %f" % (val_acc/ n_batch))

    ACC[epoch] = ac*100  
    if epoch%100 == 0:
      plt.plot(XX, ACC,color='r',marker='o',markerfacecolor='blue',markersize=5)
      plt.yticks(np.linspace(0,100,21))
      plt.show()








